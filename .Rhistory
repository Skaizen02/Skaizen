myStopwords <- c(stopwords("english"), "available", "via")
myStopwords <- setdiff(myStopwords, c("r", "big"))
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
myCorpus <- tm_map(myCorpus, stripWhitespace)
#Stemming corpus
myCorpusCopy <- myCorpus
myCorpus <- tm_map(myCorpus, stemDocument)
stemCompletion2 <- function(x, dictionary) {
x <- unlist(strsplit(as.character(x), " "))
x <- x[x != ""]
x <- stemCompletion(x, dictionary=dictionary)
x <- paste(x, sep="", collapse=" ")
PlainTextDocument(stripWhitespace(x))
}
#Stem completion
myCorpus <- lapply(myCorpus, stemCompletion2, dictionary=myCorpusCopy)
myCorpus <- lapply(myCorpus, as.character)
myCorpus <- Corpus(VectorSource(myCorpus))
#Manual word replacement
replaceWord <- function(corpus, oldword, newword) {
tm_map(corpus, content_transformer(gsub), pattern=oldword, replacement=newword)
}
tdm <- TermDocumentMatrix(myCorpus, control=list(wordLengths=c(1,Inf)))
#Identifying reccuring terms
m <- as.matrix(tdm)
termFrequency <- rowSums(as.matrix(tdm)) #Calculating frequencies
termFrequency <- subset(termFrequency, termFrequency >= 10) #Filtering
freq.df <- data.frame(term=names(termFrequency), freq=termFrequency) #To data frame
word.freq <- sort(rowSums(m), decreasing = TRUE) #Sorting
pal <- brewer.pal(9, "BuGn")[-(1:4)] #Creating color palette
wordcloud(words = names(word.freq), freq = word.freq, min.freq = 3, random.order = F, colors = pal)
par()
library(tm)
library(RColorBrewer)
library(wordcloud)
#Create corpus
myCorpus <- Corpus(VectorSource(unique(df$title)))
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
#Remove numbers and punctuation
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
#Remove stop words
myStopwords <- c(stopwords("english"), "available", "via")
myStopwords <- setdiff(myStopwords, c("r", "big"))
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
myCorpus <- tm_map(myCorpus, stripWhitespace)
#Stemming corpus
myCorpusCopy <- myCorpus
myCorpus <- tm_map(myCorpus, stemDocument)
stemCompletion2 <- function(x, dictionary) {
x <- unlist(strsplit(as.character(x), " "))
x <- x[x != ""]
x <- stemCompletion(x, dictionary=dictionary)
x <- paste(x, sep="", collapse=" ")
PlainTextDocument(stripWhitespace(x))
}
#Stem completion
myCorpus <- lapply(myCorpus, stemCompletion2, dictionary=myCorpusCopy)
myCorpus <- lapply(myCorpus, as.character)
myCorpus <- Corpus(VectorSource(myCorpus))
#Manual word replacement
replaceWord <- function(corpus, oldword, newword) {
tm_map(corpus, content_transformer(gsub), pattern=oldword, replacement=newword)
}
tdm <- TermDocumentMatrix(myCorpus, control=list(wordLengths=c(1,Inf)))
#Identifying reccuring terms
m <- as.matrix(tdm)
termFrequency <- rowSums(as.matrix(tdm)) #Calculating frequencies
termFrequency <- subset(termFrequency, termFrequency >= 10) #Filtering
freq.df <- data.frame(term=names(termFrequency), freq=termFrequency) #To data frame
word.freq <- sort(rowSums(m), decreasing = TRUE) #Sorting
pal <- brewer.pal(9, "BuGn")[-(1:4)] #Creating color palette
wordcloud(words = names(word.freq), freq = word.freq, min.freq = 3, random.order = F, colors = pal)
par(mar = c(5,4,4,2))
library(tm)
library(RColorBrewer)
library(wordcloud)
#Create corpus
myCorpus <- Corpus(VectorSource(unique(df$title)))
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
#Remove numbers and punctuation
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
#Remove stop words
myStopwords <- c(stopwords("english"), "available", "via")
myStopwords <- setdiff(myStopwords, c("r", "big"))
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
myCorpus <- tm_map(myCorpus, stripWhitespace)
#Stemming corpus
myCorpusCopy <- myCorpus
myCorpus <- tm_map(myCorpus, stemDocument)
stemCompletion2 <- function(x, dictionary) {
x <- unlist(strsplit(as.character(x), " "))
x <- x[x != ""]
x <- stemCompletion(x, dictionary=dictionary)
x <- paste(x, sep="", collapse=" ")
PlainTextDocument(stripWhitespace(x))
}
#Stem completion
myCorpus <- lapply(myCorpus, stemCompletion2, dictionary=myCorpusCopy)
myCorpus <- lapply(myCorpus, as.character)
myCorpus <- Corpus(VectorSource(myCorpus))
#Manual word replacement
replaceWord <- function(corpus, oldword, newword) {
tm_map(corpus, content_transformer(gsub), pattern=oldword, replacement=newword)
}
tdm <- TermDocumentMatrix(myCorpus, control=list(wordLengths=c(1,Inf)))
#Identifying reccuring terms
m <- as.matrix(tdm)
termFrequency <- rowSums(as.matrix(tdm)) #Calculating frequencies
termFrequency <- subset(termFrequency, termFrequency >= 10) #Filtering
freq.df <- data.frame(term=names(termFrequency), freq=termFrequency) #To data frame
word.freq <- sort(rowSums(m), decreasing = TRUE) #Sorting
pal <- brewer.pal(9, "BuGn")[-(1:4)] #Creating color palette
wordcloud(words = names(word.freq), freq = word.freq, min.freq = 3, random.order = F, colors = pal)
par(mar = rep(2,4))
library(tm)
library(RColorBrewer)
library(wordcloud)
#Create corpus
myCorpus <- Corpus(VectorSource(unique(df$title)))
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
#Remove numbers and punctuation
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
#Remove stop words
myStopwords <- c(stopwords("english"), "available", "via")
myStopwords <- setdiff(myStopwords, c("r", "big"))
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
myCorpus <- tm_map(myCorpus, stripWhitespace)
#Stemming corpus
myCorpusCopy <- myCorpus
myCorpus <- tm_map(myCorpus, stemDocument)
stemCompletion2 <- function(x, dictionary) {
x <- unlist(strsplit(as.character(x), " "))
x <- x[x != ""]
x <- stemCompletion(x, dictionary=dictionary)
x <- paste(x, sep="", collapse=" ")
PlainTextDocument(stripWhitespace(x))
}
#Stem completion
myCorpus <- lapply(myCorpus, stemCompletion2, dictionary=myCorpusCopy)
myCorpus <- lapply(myCorpus, as.character)
myCorpus <- Corpus(VectorSource(myCorpus))
#Manual word replacement
replaceWord <- function(corpus, oldword, newword) {
tm_map(corpus, content_transformer(gsub), pattern=oldword, replacement=newword)
}
tdm <- TermDocumentMatrix(myCorpus, control=list(wordLengths=c(1,Inf)))
#Identifying reccuring terms
m <- as.matrix(tdm)
termFrequency <- rowSums(as.matrix(tdm)) #Calculating frequencies
termFrequency <- subset(termFrequency, termFrequency >= 10) #Filtering
freq.df <- data.frame(term=names(termFrequency), freq=termFrequency) #To data frame
word.freq <- sort(rowSums(m), decreasing = TRUE) #Sorting
pal <- brewer.pal(9, "BuGn")[-(1:4)] #Creating color palette
wordcloud(words = names(word.freq), freq = word.freq, min.freq = 3, random.order = F, colors = pal)
links <- crawler(2, url, path)
library(Rcrawler)
library(rvest)
library(tm)
library(beepr)
url <- "https://www.finextra.com/"
path <- "https://www.finextra.com/newsarticle"
#Crawler
crawler <- function(iterations, url, path, step = 10) {
links <- list(url)
scannedLinks <- c()
for (i in 0:iterations) {
tmp <- links
for (j in seq.int(length(links))) {
if (length(scannedLinks)%%step == 0 ) {
print(sprintf("%s/%s : %s/%s", i, iterations, length(scannedLinks), length(tmp)))
}
link <- links[j][[1]]
if (!(link %in% scannedLinks)) {
pageLinks <- LinkExtractor(link)[[2]]
pageLinks <- c(lapply(pageLinks, function (x) {
if (startsWith(x, path)) {
x
}
}))
pageLinks <- pageLinks[!sapply(pageLinks, is.null)]
tmp <- c(tmp, pageLinks)
scannedLinks <- c(scannedLinks, link)
}
}
links <- unique(tmp)
}
if (!startsWith(url, path)) {
links[1] <- NULL
}
links <- lapply(links, function(x) as.character(x))
links <- unlist(links)
links
}
#Parser
selector <-   ".left.fullWidth:not(.left.fullWidth.upper.fontColorOne),
.ncMetaDataSnippet,
#ctl00_ctl00_ConMainBody_ConMainBody_ctl01_lblInfo,
#twitterResult,
#liResult,
#fbResult,
#reResult,
#goResult,
#emResult"
months <- c("January", "February", "March", "April", "May", "June", "July", "August", "September", "October", "November", "December")
parser <- function(links, limit = 0, step = 1, beep = T) {
startTime <- as.integer(Sys.time())
df <- data.frame(url = unlist(links))
df$twitter <- NA
df$linkedin <- NA
df$facebook <- NA
df$reddit <- NA
df$google <- NA
df$mail <- NA
df$title  <- NA
df$postTags <- NA
df$time <- NA
if (limit == 0) {
l <- length(links)
} else {
l <- limit
}
time <- Sys.time()
for(i in seq.int(l)) {
link <- links[[i]]
html <- read_html(link)
data <- html_text(html_nodes(html, selector))
#Social networks
df$twitter[i] <- data[1]
df$linkedin[i] <- data[2]
df$facebook[i] <- data[3]
df$reddit[i] <- data[4]
df$google[i] <- data[5]
df$mail[i] <- data[6]
#Title
df$title[i] <- data[6]
#Tags
if (length(data) > 8) {
df$postTags[i] <- paste(data[9:length(data)], collapse = ",")
}
#Stats
stats <- unlist(strsplit(data[8], "[[:space:]]"))
#Views
df$views[i] <- as.integer(stats[7])
#Comments
df$comments[i] <- as.integer(stats[12])
#Time
timestamp <- as.integer(Sys.time())
if (stats[2] == "hours" || stats[2] == "hour") {
hours <- as.integer(stats[1])
df$time[i] <- time - hours * 3600
} else if (stats[2] == "minutes" || stats[2] == "minute") {
minutes <- as.integer(stats[1])
df$time[i] <- time - minutes * 60
} else {
year <- as.integer(stats[3])
month <- which(months == stats[2])
day <- as.integer(stats[1])
date <- ISOdate(year, month, day)
df$time[i] <- as.integer(date)
}
#Progress information
if (i %% step == 0) {
avgTime <- (as.integer(Sys.time()) - startTime) / i
remainingTime <- avgTime * (l-i)
seconds <- round(remainingTime %% 60)
minutes <- round((remainingTime - seconds) / 60)
progress_bar <- paste(c("[", lapply(seq.int(25), function(x, progress) {
if (x <= progress) {
"#"
} else {
" "
}
}, round(i/l*25)), "] "), sep = "", collapse = "")
stepindicator <- sprintf("%s/%s", i, l)
timeEstimate <- sprintf(" (%s minutes %s seconds)", minutes, seconds)
print(paste(c( progress_bar, stepindicator, timeEstimate), sep = " ", collapse = ""))
}
}
df <- subset(df, !is.na(title))
df$id <- seq.int(length(df$url))
if (beep) {
beep()
}
df[1:length(df$title),]
}
go <- function() {
links <- crawler(2, url, path)
df <- parser(links, 100)
}
links <- crawler(2, url, path)
df <- parser(links, step = 10, beep = T)
library(RMySQL)
#Connection to the database
con <- dbConnect(MySQL(), user="skaizen2", password="kzm38gw35f", dbname="webcrawler", host="localhost")
#Overwrite table
dbWriteTable(conn=con, name=tablename, value=df, row.names = FALSE, overwrite = TRUE)
tablename <- "webcrawler"
#Overwrite table
dbWriteTable(conn=con, name=tablename, value=df, row.names = FALSE, overwrite = TRUE)
source('~/GitHub/Skaizen/WebCrawler.R')
df$twitter -> as.integer(df$twitter)
df$twitter <- as.integer(df$twitter)
df$linkedin -> as.integer(df$twitter)
df$linkedin <- as.integer(df$linkedin)
df$facebook <- as.integer(df$facebook)
df$reddit <- as.integer(df$reddit)
df$google <- as.integer(df$google)
df$mail <- as.integer(df$mail)
View(df)
#Overwrite table
dbWriteTable(conn=con, name=tablename, value=df, row.names = FALSE, overwrite = TRUE)
plot(df$views, df$linkedin)
plot(df$views, df$linkedin, type = "points")
plot(df$views, df$linkedin, type = "p")
plot(df$views, df$linkedin, type = "p")
qplot(df$views, df$linkedin, type = "points")
library(ggplot2)
qplot(df$views, df$linkedin, type = "points")
qplot(df$views, df$linkedin, gemo = "points")
qplot(df$views, df$linkedin, geom = "points")
qplot(df$views, df$linkedin, geom = "point")
qplot(df$views, df$linkedin, geom = "point", shape = "cross")
qplot(df$views, df$linkedin, geom = "point")
qplot(df$views, df$linkedin, geom = "point", shape = "square")
qplot(df$views, df$linkedin, geom = "point")
lm()
lm(views ~ linkedin, df)
coef(lm(views ~ linkedin, df))
coef(lm(views ~ linkedin, df))[1]
coef(lm(linkedin ~ views), df))[1]
coef(lm(linkedin ~ views, df))[1]
coef(lm(df$linkedin ~ df$views, df))[1]
coef(lm(linkedin ~ views, df))[1]
coef(lm(linkedin ~ views, df))
coef(lm(linkedin ~ views, df))[views]
coef(lm(linkedin ~ views, df))[2]
coef <- coef(lm(linkedin ~ views, df)]
coef <- coef(lm(linkedin ~ views, df)
coef <- coef(lm(linkedin ~ views, df))
coef <- coef(lm(linkedin ~ views, df)))
coef <- coef(lm(linkedin ~ views, df))
goal(linkedin ~ views, df)
xyplot(coef)
plot((1:50000)*coef[2])
qplot(df$views, df$linkedin, geom = "point") + plot((1:50000)*coef[2])
lines(50000, 50000*coef[2])
lines(50000, 50000*coef[2], col  = "green")
lines(50000, 50000*coef[2], col="green")
qplot(df$views, df$linkedin, geom = "point")
lines(50000, 50000*coef[2], col="green")
abline(coef)
coef <- coef(lm(linkedin ~ views, df))
coef
qplot(df$views, df$linkedin, geom = "point")
abline(coef)
abline(coef = coef)
plot(df$views, df$linkedin)
abline(coef = coef)
par(mar = c(1,1,1,1))
plot(df$views, df$linkedin)
par(mar = c(2,2,2,2))
plot(df$views, df$linkedin)
par(mar = c(2,2,2,2))
par(mar = c(3,3,2,2))
plot(df$views, df$linkedin)
par(mar = c(4,4,1,1))
plot(df$views, df$linkedin)
plot(df$views, df$linkedin, xlab = "Number of views", ylab = "Shares on likedin")
plot(df$views, df$linkedin, xlab = "Views", ylab = "Shares on likedin")
plot(df$views, df$linkedin, xlab = "Views", ylab = "Shares on likedin", main = "Linkedin over Views lienar model")
par(mar = c(4,4,2,1))
plot(df$views, df$linkedin, xlab = "Views", ylab = "Shares on likedin", main = "Linkedin over Views lienar model")
plot(df$views, df$linkedin, xlab = "Views", ylab = "Shares on likedin", main = "Linkedin over Views lienar model")
abline(coef = coef)
plot(df$views, df$linkedin, xlab = "Views", ylab = "Shares on likedin", main = "Linkedin over Views lienar model") + plot(df$views, df$twitter)
plot(df$views, df$linkedin, xlab = "Views", ylab = "Shares on likedin", main = "Linkedin over Views lienar model")
plot(df$views, df$linkedin, xlab = "Views", ylab = "Shares on likedin", main = "Linkedin over Views lienar model") + plot(df$views, df$twitter)
plot(df$views, df$linkedin, xlab = "Views", ylab = "Shares on likedin", main = "Linkedin over Views lienar model")
plot(df$views, df$linkedin, xlab = "Views", ylab = "Shares on likedin", main = "Linkedin over Views lienar model") + abline(coef = coef)
coef
plot(df$views, df$linkedin, xlab = "Views", ylab = "Shares on likedin", main = "Linkedin over Views lienar model") + abline(coef = coef(lm(linkedin ~ views, df)))
lm(linked ~ views, df)
lm(linkedin ~ views, df)
plot(0,5000,df$views, df$linkedin, xlab = "Views", ylab = "Shares on likedin", main = "Linkedin over Views lienar model") + abline(coef = coef(lm(linkedin ~ views, df)))
plot(xlim = c(0,50000), ylim = c(0,120),df$views, df$linkedin, xlab = "Views", ylab = "Shares on likedin", main = "Linkedin over Views lienar model") + abline(coef = coef(lm(linkedin ~ views, df)))
plot(xlim = c(0,50000), ylim = c(0,120),df$views, df$linkedin, xlab = "Views", ylab = "Shares on likedin", main = "Linkedin over Views lienar model") + abline(coef = coef(lm(linkedin ~ views, df))
)
grid()
qplot(df$views, df$linkedin, geom = "point")
abline(coef(lm(linkedin ~ views, df)))
qplot(df$views, df$linkedin, geom = "point") + grom_abline(coef(lm(linkedin ~ views, df)))
qplot(df$views, df$linkedin, geom = "point") + geom_abline(coef(lm(linkedin ~ views, df)))
qplot(df$views, df$linkedin, geom = "point") + stat_smooth(method = "lm")
qplot(df$views, df$linkedin, geom = "point") + stat_smooth(method = "lm", se  = F)
plot(xlim = c(0,50000), ylim = c(0,120),df$views, df$linkedin, xlab = "Views", ylab = "Shares on likedin", main = "Linkedin over Views lienar model") + abline(coef = coef(lm(linkedin ~ views, df)) + grid()
plot(xlim = c(0,50000), ylim = c(0,120),df$views, df$linkedin, xlab = "Views", ylab = "Shares on likedin", main = "Linkedin over Views lienar model") + abline(coef = coef(lm(linkedin ~ views, df))) + grid()
plot(xlim = c(0,50000), ylim = c(0,120),df$views, df$linkedin, xlab = "Views", ylab = "Shares on likedin", main = "Linkedin over Views lienar model") + abline(coef = coef(lm(linkedin ~ views, df))) + grid()
plot(xlim = c(0,50000), ylim = c(0,120),df$views, df$linkedin, xlab = "Views", ylab = "Shares on likedin", main = "Linkedin over Views linear model", pch = 3) + abline(coef = coef(lm(linkedin ~ views, df))) + grid()
plot(xlim = c(0,50000), ylim = c(0,120),df$views, df$linkedin, xlab = "Views", ylab = "Shares on likedin", main = "Linkedin over Views linear model", pch = 4) + abline(coef = coef(lm(linkedin ~ views, df))) + grid()
lm(df$linkedin ~ df$views)
plot(xlim = c(0,50000), ylim = c(0,120),df$views, df$linkedin, xlab = "Views", ylab = "Shares on likedin", main = "Linkedin over Views linear model", pch = 4) + abline(coef = coef(lm(linkedin ~ views, df))) + grid()
linear_model(df$views, df$linkedin)
linear_model <- function(x_data, y_data)  {
plot(x_data, y_data, main = "Linear model", pch = 4)
coef = coef(lm(y_data ~ x_data))
abline(coef = coef)
grid()
coef
}
source('~/GitHub/Skaizen/PlotModels.R')
linear_model <- function(x_data, y_data)  {
plot(x_data, y_data, main = "Linear model", pch = 4)
coef = coef(lm(y_data ~ x_data))
abline(coef = coef)
grid()
coef
}
linear_model(df$views, df$linkedin)
source('~/GitHub/Skaizen/PlotModels.R')
source('~/GitHub/Skaizen/PlotModels.R')
linear_model(df$views, df$linkedin)
source('~/GitHub/Skaizen/PlotModels.R')
linear_model(df$views, df$linkedin)
source('~/GitHub/Skaizen/PlotModels.R')
linear_model(df$views, df$linkedin)
linear_model(df$views, df$linkedin+df$twitter+df$facebook+df$reddit+df$mail+df$google)
identify(x = 36000)
linear_model(df$views, df$linkedin+df$twitter+df$facebook+df$reddit+df$mail+df$google)
identify(tolerance = Inf, n = 1)
identify(x = 36000, tolerance = Inf, n = 1)
point <- identify(x = 36000, tolerance = Inf, n = 1)
point
shares <- df$linkedin+df$twitter+df$facebook+df$reddit+df$mail+df$google
identify(x = df$views, y = shares)
identify(x = df$views, y = shares, n = 1)
linear_model(df$views, df$linkedin+df$twitter+df$facebook+df$reddit+df$mail+df$google)
identify(x = df$views, y = shares, n = 1)
df[333]
df[333,]
View(df)
df$url[333]
sum(df$google)
View(df)
df$url[263]
open(df$url[263])
browseURL(df$url[263])
df$url[263]
df$url[[263]
]
df$url[[263]]
browseURL(url = df$url[263])
df$url[263]
browseURL(url = https://www.finextra.com/newsarticle/31697/how-open-banking-will-blow-core-systems-out-of-the-water)
browseURL(url = "https://www.finextra.com/newsarticle/31697/how-open-banking-will-blow-core-systems-out-of-the-water")
df$url <- unlist(df$url)
df$url[263]
browseURL(url = df$url[263])
browseURL(url = "https://www.finextra.com/newsarticle/31697/how-open-banking-will-blow-core-systems-out-of-the-water")
length(which(df$google > 0))
coef[1]
coef[2]
coef[1] + 5
coef[1] + 5
as.numeric(coef[1])
source('~/GitHub/Skaizen/WebCrawler.R')
df$social <- df$twitter + df$linkedin + df$facebook + df$reddit + df$google + df$mail
offset <- (df$social - coef[1])/df$views - coef[2]
offset*
offset
(df$social - coef[1])/df$views
coef[2]
offset <- (df$social - coef[1])/df$views - coef[2]
offset <- odrder(offset)
offset <- offset[order(offset)]
offset <- unlist(offset)
View(offset)
offset <- data.frame(df = df$id, value = (df$social - coef[1])/df$views - coef[2])
offset <- data.frame(value = (df$social - coef[1])/df$views - coef[2])
offset <- (df$social - coef[1])/df$views - coef[2]
offset
view(offset)
View(offset)
identify(x = df$views, y = shares, n = 1)
linear_model(offset)
linear_model(x = seq.int(length(offset)), y = offset)
plot(offset)
plot(offset, pch = 4)
identify(offset, n = 1)
View(df)
offset <- df$social/df$views - (coef[1] + coef[2] * df$views)
plot(offset, pch = 4)
offset <- df$social - (coef[1] + coef[2] * df$views)
plot(offset, pch = 4)
offset <- data.frame(value = df$social - (coef[1] + coef[2] * df$views))
View(offset)
offset <- data.frame(value = (df$social-coef[1])/df$views - coef[2])
View(offset)
plot(offset, pch = 4)
offset <- data.frame(value = (df$social-coef[1])/df$views - coef[2], id = df$id)
plot(offset)
plot(x = offset$id, y = offset$value, pch = 4)
plot(x = offset$id, y = offset$value, n = 1)
identify(x = offset$id, y = offset$value, n = 1)
df$url[510]
plot(x = offset$id, y = offset$value, pch = 4)
identify(x = offset$id, y = offset$value, n = 1)
links <- crawler(3, url, path)
links <- crawler(3, url, path)
links <- crawler(2, url, path)
