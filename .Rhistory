optimize = "sgd",
metrics = c("accuracy")
)
history <- model %>% fit(
x_train, y_train,
epochs = 30, batch_size = 128,
validation_split = 0.2
)
library(keras)
library(lime)
library(dplyr)
library(ggplot)
library(tidyquant)
library(rsample)
data_tbl <- df
set.seed(100)
train_test_split <- initial_split(data_tbl, prop = 0.8)
train_test_split
train_tbl <- training(train_test_split)
test_tbl  <- testing(train_test_split)
x_train <- select(train_tbl, -c("views")) %>% as.matrix()
x_test <- select(test_tbl, -c("views")) %>% as.matrix()
y_train <- train_tbl$views
y_test <- test_tbl$views
model <- keras_model_sequential()
model %>%
layer_dense(units = round(ncol(x_train)*1.2), activation = "relu", input_shape = c(ncol(x_train))) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 1, activation = "relu")
summary(model)
model %>% compile(
loss = "mean_absolute_error",
optimize = "sgd",
metrics = c("accuracy")
)
history <- model %>% fit(
x_train, y_train,
epochs = 30, batch_size = 128,
validation_split = 0.2
)
plot(history)
model %>% evaluate(x_test, y_test)
# Setup lime::model_type() function for keras
model_type.keras.models.Sequential <- function(x, ...) {
return("classification")
}
# Setup lime::predict_model() function for keras
predict_model.keras.models.Sequential <- function(x, newdata, type, ...) {
pred <- predict_proba(object = x, x = as.matrix(newdata))
return(data.frame(Yes = pred, No = 1 - pred))
}
# Test our predict_model() function
predict_model(x = model_keras, newdata = x_test_tbl, type = 'raw') %>%
tibble::as_tibble()
# Run lime() on training set
explainer <- lime::lime(
x              = x_train_tbl,
model          = model_keras,
bin_continuous = FALSE)
# Run explain() on explainer
explanation <- lime::explain(
x_test_tbl[1:10,],
explainer    = explainer,
n_labels     = 1,
n_features   = 4,
kernel_width = 0.5)
plot_features(explanation) +
labs(title = "LIME Feature Importance Visualization",
subtitle = "Hold Out (Test) Set, First 10 Cases Shown")
plot_explanations(explanation) +
labs(title = "LIME Feature Importance Heatmap",
subtitle = "Hold Out (Test) Set, First 10 Cases Shown")
# Feature correlations to Churn
corrr_analysis <- data.frame(x_train) %>%
mutate(views = y_train) %>%
correlate() %>%
focus(views) %>%
rename(feature = rowname) %>%
arrange(abs(views)) %>%
mutate(feature = as_factor(feature))
corrr_analysis
# Correlation visualization
corrr_analysis %>%
ggplot(aes(x = views, y = fct_reorder(feature, desc(views)))) +
geom_point() +
# Positive Correlations - more views
geom_segment(aes(xend = 0, yend = feature),
color = palette_light()[[2]],
data = corrr_analysis %>% filter(views > 0)) +
geom_point(color = palette_light()[[2]],
data = corrr_analysis %>% filter(views > 0)) +
# Negative Correlations - less views
geom_segment(aes(xend = 0, yend = feature),
color = palette_light()[[1]],
data = corrr_analysis %>% filter(views < 0)) +
geom_point(color = palette_light()[[1]],
data = corrr_analysis %>% filter(views < 0)) +
# Vertical lines
geom_vline(xintercept = 0, color = palette_light()[[5]], size = 1, linetype = 2) +
geom_vline(xintercept = -0.25, color = palette_light()[[5]], size = 1, linetype = 2) +
geom_vline(xintercept = 0.25, color = palette_light()[[5]], size = 1, linetype = 2) +
# Aesthetics
theme_tq() +
labs(title = "Keywords/Views Correlation Analysis",
subtitle = "Positive Correlations (more views), Negative Correlations (less views)",
y = "Feature Importance") +
theme(text = element_text(size=8))
colnames(x_train)
colanmes(x_test)
colnames(x_test)
library(tibble)
data_tbl <- df
set.seed(100)
train_test_split <- initial_split(data_tbl, prop = 0.8)
train_test_split
train_tbl <- training(train_test_split)
test_tbl  <- testing(train_test_split)
x_train <- select(train_tbl, -c("views")) %>% as.matrix()
x_test <- select(test_tbl, -c("views")) %>% as.matrix()
y_train <- train_tbl$views
y_test <- test_tbl$views
model <- keras_model_sequential()
model %>%
layer_dense(units = round(ncol(x_train)*1.2), activation = "relu", input_shape = c(ncol(x_train))) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 1, activation = "relu")
summary(model)
model %>% compile(
loss = "mean_absolute_error",
optimize = "sgd",
metrics = c("accuracy")
)
history <- model %>% fit(
x_train, y_train,
epochs = 30, batch_size = 128,
validation_split = 0.2
)
colanmes(x_train)
colnames(x_train)
plot(history)
model %>% evaluate(x_test, y_test)
# Setup lime::model_type() function for keras
model_type.keras.models.Sequential <- function(x, ...) {
return("classification")
}
# Setup lime::predict_model() function for keras
predict_model.keras.models.Sequential <- function(x, newdata, type, ...) {
pred <- predict_proba(object = x, x = as.matrix(newdata))
return(data.frame(Yes = pred, No = 1 - pred))
}
# Test our predict_model() function
predict_model(x = model_keras, newdata = x_test_tbl, type = 'raw') %>%
tibble::as_tibble()
library(keras)
library(lime)
library(tidyquant)
library(rsample)
library(recipes)
library(yardstick)
library(corrr)
# Test our predict_model() function
predict_model(x = model_keras, newdata = x_test_tbl, type = 'raw') %>%
tibble::as_tibble()
x_test_tbl
# Test our predict_model() function
predict_model(x = model_keras, newdata = x_test, type = 'raw') %>%
tibble::as_tibble()
df <- read.csv(df2.csv)
df <- read.csv("df2.csv")
setwd("C:/Users/robin/Desktop/workspace/GitHub/Skaizen")
df <- read.csv("df2.csv")
df <- preprocessing(df)
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/Preprocessing.R')
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/Preprocessing.R')
df <- preprocessing(df)
View(df)
strsplti(df$tags, ",")
strsplit(df$tags, ",")
strsplit(df$postTags, ",")
df$postTags
typeof(df$postTags)
as.character(df$postTags)
#Identifying tags
tagList <- unique(unlist(strsplit(as.character(df$postTags), ",")))
library(tm)
library(SnowballC)
library(dplyr)
library(keras)
preprocessing <- function(df) {
df <- select(df, c("views", "text", "time", "postTags"))
#Identifying keywords
docs = Corpus(VectorSource(df$text))
docs = tm_map(docs, removePunctuation)
docs = tm_map(docs, removeNumbers)
docs = tm_map(docs, tolower)
docs = tm_map(docs, removeWords, stopwords("english"))
docs = tm_map(docs, stripWhitespace)
docs = tm_map(docs, stemDocument)
dtm = DocumentTermMatrix(docs)
dtmdf = data.frame(as.matrix(dtm))
freq = colSums(as.matrix(dtm))
dtmdf = dtmdf[,order(freq, decreasing = T)[1:128]]
dtmdf[] = lapply(dtmdf, function(x) ifelse(x>0, 1, 0))
colnames(dtmdf) = paste("word", colnames(dtmdf), sep = "_")
df = cbind(df, dtmdf)
#Identifying tags
tagList <- unique(unlist(strsplit(as.character(df$postTags), ",")))
tagdf <- NULL
tagdf <- data.frame(lapply(tagList, function(tag){
unlist(lapply(df$postTags, function(postTags){
ifelse(tag %in% unlist(strsplit(postTags, ",")), 1, 0)
}))
}))
colnames(tagdf) <- paste("tag", tagList, sep = "_")
df <- cbind(df, tagdf)
#Identifying week days
wdays = as.POSIXlt(df$time, origin = "1970-01-01")$wday
wdaydf <- data.frame(to_categorical(wdays, 7))
colnames(wdaydf) <- c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")
df <- cbind(df, wdaydf)
select(df, -c("text", "time", "postTags"))
}
df <- preprocessing(df)
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/Preprocessing.R')
df$postTags <- as.character(df$postTags)
df <- preprocessing(df)
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/MachineLearning.R')
library(keras)
library(lime)
library(dplyr)
library(ggplot)
library(tidyquant)
library(rsample)
data_tbl <- df
set.seed(100)
train_test_split <- initial_split(data_tbl, prop = 0.8)
train_test_split
train_tbl <- training(train_test_split)
test_tbl  <- testing(train_test_split)
x_train <- select(train_tbl, -c("views")) %>% as.matrix()
x_test <- select(test_tbl, -c("views")) %>% as.matrix()
y_train <- train_tbl$views
y_test <- test_tbl$views
model <- keras_model_sequential()
model %>%
layer_dense(units = round(ncol(x_train)*1.2), activation = "relu", input_shape = c(ncol(x_train))) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 1, activation = "relu")
summary(model)
model %>% compile(
loss = "mean_absolute_error",
optimize = "sgd",
metrics = c("accuracy")
)
history <- model %>% fit(
x_train, y_train,
epochs = 30, batch_size = 128,
validation_split = 0.2
)
plot(history)
model %>% evaluate(x_test, y_test)
# Setup lime::model_type() function for keras
model_type.keras.models.Sequential <- function(x, ...) {
return("classification")
}
# Setup lime::predict_model() function for keras
predict_model.keras.models.Sequential <- function(x, newdata, type, ...) {
pred <- predict_proba(object = x, x = as.matrix(newdata))
return(data.frame(Yes = pred, No = 1 - pred))
}
# Test our predict_model() function
predict_model(x = model_keras, newdata = x_test, type = 'raw') %>%
tibble::as_tibble()
# Run lime() on training set
explainer <- lime::lime(
x              = x_train_tbl,
model          = model_keras,
bin_continuous = FALSE)
# Run explain() on explainer
explanation <- lime::explain(
x_test_tbl[1:10,],
explainer    = explainer,
n_labels     = 1,
n_features   = 4,
kernel_width = 0.5)
plot_features(explanation) +
labs(title = "LIME Feature Importance Visualization",
subtitle = "Hold Out (Test) Set, First 10 Cases Shown")
plot_explanations(explanation) +
labs(title = "LIME Feature Importance Heatmap",
subtitle = "Hold Out (Test) Set, First 10 Cases Shown")
# Feature correlations to Churn
corrr_analysis <- data.frame(x_train) %>%
mutate(views = y_train) %>%
correlate() %>%
focus(views) %>%
rename(feature = rowname) %>%
arrange(abs(views)) %>%
mutate(feature = as_factor(feature))
corrr_analysis
# Correlation visualization
corrr_analysis %>%
ggplot(aes(x = views, y = fct_reorder(feature, desc(views)))) +
geom_point() +
# Positive Correlations - more views
geom_segment(aes(xend = 0, yend = feature),
color = palette_light()[[2]],
data = corrr_analysis %>% filter(views > 0)) +
geom_point(color = palette_light()[[2]],
data = corrr_analysis %>% filter(views > 0)) +
# Negative Correlations - less views
geom_segment(aes(xend = 0, yend = feature),
color = palette_light()[[1]],
data = corrr_analysis %>% filter(views < 0)) +
geom_point(color = palette_light()[[1]],
data = corrr_analysis %>% filter(views < 0)) +
# Vertical lines
geom_vline(xintercept = 0, color = palette_light()[[5]], size = 1, linetype = 2) +
geom_vline(xintercept = -0.25, color = palette_light()[[5]], size = 1, linetype = 2) +
geom_vline(xintercept = 0.25, color = palette_light()[[5]], size = 1, linetype = 2) +
# Aesthetics
theme_tq() +
labs(title = "Keywords/Views Correlation Analysis",
subtitle = "Positive Correlations (more views), Negative Correlations (less views)",
y = "Feature Importance") +
theme(text = element_text(size=8))
# Correlation visualization
corrr_analysis %>%
ggplot(aes(x = views, y = fct_reorder(feature, desc(views)))) +
geom_point() +
# Positive Correlations - more views
geom_segment(aes(xend = 0, yend = feature),
color = palette_light()[[2]],
data = corrr_analysis %>% filter(views > 0)) +
geom_point(color = palette_light()[[2]],
data = corrr_analysis %>% filter(views > 0)) +
# Negative Correlations - less views
geom_segment(aes(xend = 0, yend = feature),
color = palette_light()[[1]],
data = corrr_analysis %>% filter(views < 0)) +
geom_point(color = palette_light()[[1]],
data = corrr_analysis %>% filter(views < 0)) +
# Vertical lines
geom_vline(xintercept = 0, color = palette_light()[[5]], size = 1, linetype = 2) +
geom_vline(xintercept = -0.25, color = palette_light()[[5]], size = 1, linetype = 2) +
geom_vline(xintercept = 0.25, color = palette_light()[[5]], size = 1, linetype = 2) +
# Aesthetics
theme_tq() +
labs(title = "Keywords/Views Correlation Analysis",
subtitle = "Positive Correlations (more views), Negative Correlations (less views)",
y = "Feature Importance") +
theme(text = element_text(size=5))
corrr_analysis %>%
ggplot(aes(x = views, y = fct_reorder(feature, desc(views)))) +
geom_point() +
# Positive Correlations - more views
geom_segment(aes(xend = 0, yend = feature),
color = palette_light()[[2]],
data = corrr_analysis %>% filter(views > 0)) +
geom_point(color = palette_light()[[2]],
data = corrr_analysis %>% filter(views > 0)) +
# Negative Correlations - less views
geom_segment(aes(xend = 0, yend = feature),
color = palette_light()[[1]],
data = corrr_analysis %>% filter(views < 0)) +
geom_point(color = palette_light()[[1]],
data = corrr_analysis %>% filter(views < 0)) +
# Vertical lines
geom_vline(xintercept = 0, color = palette_light()[[5]], size = 1, linetype = 2) +
geom_vline(xintercept = -0.25, color = palette_light()[[5]], size = 1, linetype = 2) +
geom_vline(xintercept = 0.25, color = palette_light()[[5]], size = 1, linetype = 2) +
# Aesthetics
theme_tq() +
labs(title = "Keywords/Views Correlation Analysis",
subtitle = "Positive Correlations (more views), Negative Correlations (less views)",
y = "Feature Importance") +
theme(text = element_text(size=7))
corrr_analysis %>%
ggplot(aes(x = views, y = fct_reorder(feature, desc(views)))) +
geom_point() +
# Positive Correlations - more views
geom_segment(aes(xend = 0, yend = feature),
color = palette_light()[[2]],
data = corrr_analysis %>% filter(views > 0)) +
geom_point(color = palette_light()[[2]],
data = corrr_analysis %>% filter(views > 0)) +
# Negative Correlations - less views
geom_segment(aes(xend = 0, yend = feature),
color = palette_light()[[1]],
data = corrr_analysis %>% filter(views < 0)) +
geom_point(color = palette_light()[[1]],
data = corrr_analysis %>% filter(views < 0)) +
# Vertical lines
geom_vline(xintercept = 0, color = palette_light()[[5]], size = 1, linetype = 2) +
geom_vline(xintercept = -0.25, color = palette_light()[[5]], size = 1, linetype = 2) +
geom_vline(xintercept = 0.25, color = palette_light()[[5]], size = 1, linetype = 2) +
# Aesthetics
theme_tq() +
labs(title = "Keywords/Views Correlation Analysis",
subtitle = "Positive Correlations (more views), Negative Correlations (less views)",
y = "Feature Importance") +
theme(text = element_text(size=15))
corr_analysis
corrr_analysis
View(corrr_analysis)
write(corrr_analysis, "corr.csv")
write.csv(corrr_analysis, "corr.csv")
links <- craler(2, "https://wwW.finextra.com", "/newsarticle", n=20)
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/WebCrawler.R')
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/WebCrawler.R')
links <- crawler(2, "https://wwW.finextra.com", "/newsarticle", n=20)
df <- parser(links)
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/WebCrawler.R')
df <- parser(links)
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/WebCrawler.R')
df <- parser(links)
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/WebCrawler.R')
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/WebCrawler.R')
df <- parser(links)
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/WebCrawler.R')
df <- parser(links)
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/WebCrawler.R')
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/WebCrawler.R')
df <- parser(links)
df <- parser(links)
View(stats)
stats <- unlist(stats)
debugSource('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/WebCrawler.R')
df <- parser(links)
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/WebCrawler.R')
df <- parser(links)
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/WebCrawler.R')
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/WebCrawler.R')
df <- parser(links)
install.packages("pogress")
library("progress", lib.loc="~/R/win-library/3.5")
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/WebCrawler.R')
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/WebCrawler.R')
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/WebCrawler.R')
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/WebCrawler.R')
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/WebCrawler.R')
df <- parser(links)
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/WebCrawler.R')
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/WebCrawler.R')
links <- crawler(4, "https://www.finextra.com", "/newsarticle", n = 256)
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/WebCrawler.R')
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/WebCrawler.R')
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/WebCrawler.R')
beep = T
if beep beep()
if (beep) beep()
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/WebCrawler.R')
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/WebCrawler.R')
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/WebCrawler.R')
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/WebCrawler.R')
links <- crawler(4, "https://www.finextra.com", "/newsarticle", n = 256)
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/WebCrawler.R')
links <- crawler(4, "https://www.finextra.com", "/newsarticle", n = 256)
pb <- progress_bar$new(total = n, format = "[:bar] :percent : :elapsed/:eta")
pb <- progress_bar$new(total = n, format = "[:bar] :percent : :elapsed/:eta")
n = 100
pb <- progress_bar$new(total = n, format = "[:bar] :percent : :elapsed/:eta")
pb$self
pb$completed
pb$finished
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/WebCrawler.R')
pb <- progress_bar$new(total = n, format = "[:bar] :percent : :elapsed/:eta")
links <- crawler(4, "https://www.finextra.com", "/newsarticle", n = 256)
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/WebCrawler.R')
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/WebCrawler.R')
links <- crawler(4, "https://www.finextra.com", "/newsarticle", n = 256)
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/WebCrawler.R')
links <- crawler(4, "https://www.finextra.com", "/newsarticle", n = 256)
df <- parser(links, beep = T)
links <- crawler(4, "https://www.finextra.com", "/newsarticle", n = 512)
df <- parser(links, beep = T)
df <- parser(links, beep = T)
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/WebCrawler.R')
df <- parser(links, beep = T)
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/WebCrawler.R')
df <- parser(links, beep = T)
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/WebCrawler.R')
df <- parser(links, beep = T)
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/WebCrawler.R')
df <- parser(links, beep = T)
df <- parser(links, beep = T)
pb <- progress_bar$new(total = l, format = ":spin [:bar] :percent : :elapsed/:eta(:tick_rate/:total)")
pb$tick
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/WebCrawler.R')
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/WebCrawler.R')
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/WebCrawler.R')
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/WebCrawler.R')
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/WebCrawler.R')
df <- parser(links, beep = T)
df <- parser(links, beep = T)
df <- parser(links, beep = T)
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/WebCrawler.R')
View(df)
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/WebCrawler.R')
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/WebCrawler.R')
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/WebCrawler.R')
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/WebCrawler.R')
df <- parse(links, limit=16)
df <- parser(links, limit = 16)
source('C:/Users/robin/Desktop/workspace/GitHub/Skaizen/Final/WebCrawler.R')
View(df)
df <- parser(links, beep = T)
df <- parser(links, beep = T)
write.csv(df, "data.csv")
write.csv(links, "links.csv")
save.image("C:/Users/robin/Desktop/workspace/GitHub/Skaizen/datasets/dataset5.RData")
df = parser(links, beep = T)
