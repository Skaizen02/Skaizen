sentiments$date <- as.character(as.POSIXlt(df$time, origin = "1970-01-01"))
result <- aggregate(score ~ date, data = sentiments, sum)
plot(result, type = "l")
dtm <- as.DocumentTermMatrix(tdm)
lda <- LDA(dtm, k = 8)
term <- terms(lda, 7)
(term <- apply(term, MARGIN = 2, paste, collapse = ", "))
topics <- topics(lda)
topics <- data.frame(date=as.Date(df$created), topic=topics)
ggplot(topics, aes(date, fill = term[topic])) + geom_density(position = "stack")
require(devtools)
install_github("okugami79/sentiment140", force = TRUE)
library(topicmodels)
library(sentimentr)
library(ggplot2)
sentiments <- sentiment(df$title)
table(sentiments$polarity)
sentiments$score <- 0
sentiments$score[sentiments$polarity == "positive"] <- 1
sentiments$score[sentiments$polarity == "negative"] <- -1
sentiments$date <- as.character(as.POSIXlt(df$time, origin = "1970-01-01"))
result <- aggregate(score ~ date, data = sentiments, sum)
plot(result, type = "l")
dtm <- as.DocumentTermMatrix(tdm)
lda <- LDA(dtm, k = 8)
term <- terms(lda, 7)
(term <- apply(term, MARGIN = 2, paste, collapse = ", "))
topics <- topics(lda)
topics <- data.frame(date=as.Date(df$created), topic=topics)
ggplot(topics, aes(date, fill = term[topic])) + geom_density(position = "stack")
install.packages("topicmodels")
install.packages("devtools")
library(topicmodels)
require(devtools)
install_github("okugami79/sentiment140", force = TRUE)
require(devtools)
install_github("okugami79/sentiment140", force = TRUE)
library(topicmodels)
library(sentiment)
library(ggplot2)
sentiments <- sentiment(df$title)
table(sentiments$polarity)
sentiments$score <- 0
sentiments$score[sentiments$polarity == "positive"] <- 1
sentiments$score[sentiments$polarity == "negative"] <- -1
sentiments$date <- as.character(as.POSIXlt(df$time, origin = "1970-01-01"))
result <- aggregate(score ~ date, data = sentiments, sum)
plot(result, type = "l")
dtm <- as.DocumentTermMatrix(tdm)
lda <- LDA(dtm, k = 8)
term <- terms(lda, 7)
(term <- apply(term, MARGIN = 2, paste, collapse = ", "))
topics <- topics(lda)
topics <- data.frame(date=as.Date(df$created), topic=topics)
ggplot(topics, aes(date, fill = term[topic])) + geom_density(position = "stack")
source('~/GitHub/Skaizen/WebCrawler.R')
source('~/GitHub/Skaizen/WebCrawler.R')
selector <-   ".left.fullWidth:not(.left.fullWidth.upper.fontColorOne),
.ncMetaDataSnippet,
#ctl00_ctl00_ConMainBody_ConMainBody_ctl01_lblInfo,
#twitterResult,
#fbResult,
#reResult,
#goResult,
#emResult"
html <- read_html(links[5])
nodes <- html_node(html, selector)
hmtl_text(nodes)
html_text(nodes)
links[5]
nodes <- html_nodes(html, selector)
html_text(nodes)
debugSource('~/GitHub/Skaizen/WebCrawler.R')
source('~/GitHub/Skaizen/WebCrawler.R')
source('~/GitHub/Skaizen/WebCrawler.R')
source('~/GitHub/Skaizen/WebCrawler.R')
source('~/GitHub/Skaizen/WebCrawler.R')
source('~/GitHub/Skaizen/WebCrawler.R')
source('~/GitHub/Skaizen/WebCrawler.R')
source('~/GitHub/Skaizen/WebCrawler.R')
links <- crawler(2, url, path)
source('~/GitHub/Skaizen/WebCrawler.R')
go()
View(df)
View(df)
source('~/GitHub/Skaizen/WebCrawler.R')
df <- parser(links, 5)
View(tagsSubset)
source('~/GitHub/Skaizen/WebCrawler.R')
debugSource('~/GitHub/Skaizen/WebCrawler.R')
df <- parser(links, 5)
View(df)
View(df)
View(df)
bonjour[1] <- 6
test <- data.frame()
test$tags[1] <- "wow"
test <- data.frame(tags = ("wow"))
test$tags[2] <- "waw"
debugSource('~/GitHub/Skaizen/WebCrawler.R')
debugSource('~/GitHub/Skaizen/WebCrawler.R')
df <- parser(links, 2)
df <- parser(links, 2)
source('~/GitHub/Skaizen/WebCrawler.R')
df <- parser(links, 2)
df <- parser(links, 2)
View(df)
View(df)
View(df)
data
debugSource('~/GitHub/Skaizen/WebCrawler.R')
df <- parser(links, 2)
View(df)
debugSource('~/GitHub/Skaizen/WebCrawler.R')
df <- parser(links, 2)
View(df)
View(df)
stats
data[8]
#Stats
stats <- unlist(strsplit(data[8], "[[:space:]]"))
stats
length(stats)
df <- parser(links, 2)
debugSource('~/GitHub/Skaizen/WebCrawler.R')
df <- parser(links, 2)
startTime <- as.integer(Sys.time())
df <- parser(links, 2)
hourzs
hours
year
stats
debugSource('~/GitHub/Skaizen/WebCrawler.R')
df <- parser(links, 2)
df <- parser(links, 2)
df <- parser(links, 2)
source('~/GitHub/Skaizen/WebCrawler.R')
df <- parser(links, 2)
df <- parser(links, 2)
stats
stats
debugSource('~/GitHub/Skaizen/WebCrawler.R')
debugSource('~/GitHub/Skaizen/WebCrawler.R')
df <- parser(links, 2)
df <- parser(links, 300, 10)
View(df)
library(tm)
library(RColorBrewer)
library(wordcloud)
#Create corpus
myCorpus <- Corpus(VectorSource(unique(df$title)))
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
#Remove numbers and punctuation
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
#Remove stop words
myStopwords <- c(stopwords("english"), "available", "via")
myStopwords <- setdiff(myStopwords, c("r", "big"))
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
myCorpus <- tm_map(myCorpus, stripWhitespace)
#Stemming corpus
myCorpusCopy <- myCorpus
myCorpus <- tm_map(myCorpus, stemDocument)
stemCompletion2 <- function(x, dictionary) {
x <- unlist(strsplit(as.character(x), " "))
x <- x[x != ""]
x <- stemCompletion(x, dictionary=dictionary)
x <- paste(x, sep="", collapse=" ")
PlainTextDocument(stripWhitespace(x))
}
#Stem completion
myCorpus <- lapply(myCorpus, stemCompletion2, dictionary=myCorpusCopy)
myCorpus <- lapply(myCorpus, as.character)
myCorpus <- Corpus(VectorSource(myCorpus))
#Manual word replacement
replaceWord <- function(corpus, oldword, newword) {
tm_map(corpus, content_transformer(gsub), pattern=oldword, replacement=newword)
}
tdm <- TermDocumentMatrix(myCorpus, control=list(wordLengths=c(1,Inf)))
#Identifying reccuring terms
m <- as.matrix(tdm)
termFrequency <- rowSums(as.matrix(tdm)) #Calculating frequencies
termFrequency <- subset(termFrequency, termFrequency >= 10) #Filtering
freq.df <- data.frame(term=names(termFrequency), freq=termFrequency) #To data frame
word.freq <- sort(rowSums(m), decreasing = TRUE) #Sorting
pal <- brewer.pal(9, "BuGn")[-(1:4)] #Creating color palette
wordcloud(words = names(word.freq), freq = word.freq, min.freq = 3, random.order = F, colors = pal)
install.packages("wordcloud")
library(tm)
library(RColorBrewer)
library(wordcloud)
#Create corpus
myCorpus <- Corpus(VectorSource(unique(df$title)))
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
#Remove numbers and punctuation
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
#Remove stop words
myStopwords <- c(stopwords("english"), "available", "via")
myStopwords <- setdiff(myStopwords, c("r", "big"))
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
myCorpus <- tm_map(myCorpus, stripWhitespace)
#Stemming corpus
myCorpusCopy <- myCorpus
myCorpus <- tm_map(myCorpus, stemDocument)
stemCompletion2 <- function(x, dictionary) {
x <- unlist(strsplit(as.character(x), " "))
x <- x[x != ""]
x <- stemCompletion(x, dictionary=dictionary)
x <- paste(x, sep="", collapse=" ")
PlainTextDocument(stripWhitespace(x))
}
#Stem completion
myCorpus <- lapply(myCorpus, stemCompletion2, dictionary=myCorpusCopy)
myCorpus <- lapply(myCorpus, as.character)
myCorpus <- Corpus(VectorSource(myCorpus))
#Manual word replacement
replaceWord <- function(corpus, oldword, newword) {
tm_map(corpus, content_transformer(gsub), pattern=oldword, replacement=newword)
}
tdm <- TermDocumentMatrix(myCorpus, control=list(wordLengths=c(1,Inf)))
#Identifying reccuring terms
m <- as.matrix(tdm)
termFrequency <- rowSums(as.matrix(tdm)) #Calculating frequencies
termFrequency <- subset(termFrequency, termFrequency >= 10) #Filtering
freq.df <- data.frame(term=names(termFrequency), freq=termFrequency) #To data frame
word.freq <- sort(rowSums(m), decreasing = TRUE) #Sorting
pal <- brewer.pal(9, "BuGn")[-(1:4)] #Creating color palette
wordcloud(words = names(word.freq), freq = word.freq, min.freq = 3, random.order = F, colors = pal)
par()
library(tm)
library(RColorBrewer)
library(wordcloud)
#Create corpus
myCorpus <- Corpus(VectorSource(unique(df$title)))
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
#Remove numbers and punctuation
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
#Remove stop words
myStopwords <- c(stopwords("english"), "available", "via")
myStopwords <- setdiff(myStopwords, c("r", "big"))
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
myCorpus <- tm_map(myCorpus, stripWhitespace)
#Stemming corpus
myCorpusCopy <- myCorpus
myCorpus <- tm_map(myCorpus, stemDocument)
stemCompletion2 <- function(x, dictionary) {
x <- unlist(strsplit(as.character(x), " "))
x <- x[x != ""]
x <- stemCompletion(x, dictionary=dictionary)
x <- paste(x, sep="", collapse=" ")
PlainTextDocument(stripWhitespace(x))
}
#Stem completion
myCorpus <- lapply(myCorpus, stemCompletion2, dictionary=myCorpusCopy)
myCorpus <- lapply(myCorpus, as.character)
myCorpus <- Corpus(VectorSource(myCorpus))
#Manual word replacement
replaceWord <- function(corpus, oldword, newword) {
tm_map(corpus, content_transformer(gsub), pattern=oldword, replacement=newword)
}
tdm <- TermDocumentMatrix(myCorpus, control=list(wordLengths=c(1,Inf)))
#Identifying reccuring terms
m <- as.matrix(tdm)
termFrequency <- rowSums(as.matrix(tdm)) #Calculating frequencies
termFrequency <- subset(termFrequency, termFrequency >= 10) #Filtering
freq.df <- data.frame(term=names(termFrequency), freq=termFrequency) #To data frame
word.freq <- sort(rowSums(m), decreasing = TRUE) #Sorting
pal <- brewer.pal(9, "BuGn")[-(1:4)] #Creating color palette
wordcloud(words = names(word.freq), freq = word.freq, min.freq = 3, random.order = F, colors = pal)
par(mar = c(5,4,4,2))
library(tm)
library(RColorBrewer)
library(wordcloud)
#Create corpus
myCorpus <- Corpus(VectorSource(unique(df$title)))
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
#Remove numbers and punctuation
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
#Remove stop words
myStopwords <- c(stopwords("english"), "available", "via")
myStopwords <- setdiff(myStopwords, c("r", "big"))
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
myCorpus <- tm_map(myCorpus, stripWhitespace)
#Stemming corpus
myCorpusCopy <- myCorpus
myCorpus <- tm_map(myCorpus, stemDocument)
stemCompletion2 <- function(x, dictionary) {
x <- unlist(strsplit(as.character(x), " "))
x <- x[x != ""]
x <- stemCompletion(x, dictionary=dictionary)
x <- paste(x, sep="", collapse=" ")
PlainTextDocument(stripWhitespace(x))
}
#Stem completion
myCorpus <- lapply(myCorpus, stemCompletion2, dictionary=myCorpusCopy)
myCorpus <- lapply(myCorpus, as.character)
myCorpus <- Corpus(VectorSource(myCorpus))
#Manual word replacement
replaceWord <- function(corpus, oldword, newword) {
tm_map(corpus, content_transformer(gsub), pattern=oldword, replacement=newword)
}
tdm <- TermDocumentMatrix(myCorpus, control=list(wordLengths=c(1,Inf)))
#Identifying reccuring terms
m <- as.matrix(tdm)
termFrequency <- rowSums(as.matrix(tdm)) #Calculating frequencies
termFrequency <- subset(termFrequency, termFrequency >= 10) #Filtering
freq.df <- data.frame(term=names(termFrequency), freq=termFrequency) #To data frame
word.freq <- sort(rowSums(m), decreasing = TRUE) #Sorting
pal <- brewer.pal(9, "BuGn")[-(1:4)] #Creating color palette
wordcloud(words = names(word.freq), freq = word.freq, min.freq = 3, random.order = F, colors = pal)
par(mar = rep(2,4))
library(tm)
library(RColorBrewer)
library(wordcloud)
#Create corpus
myCorpus <- Corpus(VectorSource(unique(df$title)))
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
#Remove numbers and punctuation
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
#Remove stop words
myStopwords <- c(stopwords("english"), "available", "via")
myStopwords <- setdiff(myStopwords, c("r", "big"))
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
myCorpus <- tm_map(myCorpus, stripWhitespace)
#Stemming corpus
myCorpusCopy <- myCorpus
myCorpus <- tm_map(myCorpus, stemDocument)
stemCompletion2 <- function(x, dictionary) {
x <- unlist(strsplit(as.character(x), " "))
x <- x[x != ""]
x <- stemCompletion(x, dictionary=dictionary)
x <- paste(x, sep="", collapse=" ")
PlainTextDocument(stripWhitespace(x))
}
#Stem completion
myCorpus <- lapply(myCorpus, stemCompletion2, dictionary=myCorpusCopy)
myCorpus <- lapply(myCorpus, as.character)
myCorpus <- Corpus(VectorSource(myCorpus))
#Manual word replacement
replaceWord <- function(corpus, oldword, newword) {
tm_map(corpus, content_transformer(gsub), pattern=oldword, replacement=newword)
}
tdm <- TermDocumentMatrix(myCorpus, control=list(wordLengths=c(1,Inf)))
#Identifying reccuring terms
m <- as.matrix(tdm)
termFrequency <- rowSums(as.matrix(tdm)) #Calculating frequencies
termFrequency <- subset(termFrequency, termFrequency >= 10) #Filtering
freq.df <- data.frame(term=names(termFrequency), freq=termFrequency) #To data frame
word.freq <- sort(rowSums(m), decreasing = TRUE) #Sorting
pal <- brewer.pal(9, "BuGn")[-(1:4)] #Creating color palette
wordcloud(words = names(word.freq), freq = word.freq, min.freq = 3, random.order = F, colors = pal)
links <- crawler(2, url, path)
library(Rcrawler)
library(rvest)
library(tm)
library(beepr)
url <- "https://www.finextra.com/"
path <- "https://www.finextra.com/newsarticle"
#Crawler
crawler <- function(iterations, url, path, step = 10) {
links <- list(url)
scannedLinks <- c()
for (i in 0:iterations) {
tmp <- links
for (j in seq.int(length(links))) {
if (length(scannedLinks)%%step == 0 ) {
print(sprintf("%s/%s : %s/%s", i, iterations, length(scannedLinks), length(tmp)))
}
link <- links[j][[1]]
if (!(link %in% scannedLinks)) {
pageLinks <- LinkExtractor(link)[[2]]
pageLinks <- c(lapply(pageLinks, function (x) {
if (startsWith(x, path)) {
x
}
}))
pageLinks <- pageLinks[!sapply(pageLinks, is.null)]
tmp <- c(tmp, pageLinks)
scannedLinks <- c(scannedLinks, link)
}
}
links <- unique(tmp)
}
if (!startsWith(url, path)) {
links[1] <- NULL
}
links <- lapply(links, function(x) as.character(x))
links <- unlist(links)
links
}
#Parser
selector <-   ".left.fullWidth:not(.left.fullWidth.upper.fontColorOne),
.ncMetaDataSnippet,
#ctl00_ctl00_ConMainBody_ConMainBody_ctl01_lblInfo,
#twitterResult,
#liResult,
#fbResult,
#reResult,
#goResult,
#emResult"
months <- c("January", "February", "March", "April", "May", "June", "July", "August", "September", "October", "November", "December")
parser <- function(links, limit = 0, step = 1, beep = T) {
startTime <- as.integer(Sys.time())
df <- data.frame(url = unlist(links))
df$twitter <- NA
df$linkedin <- NA
df$facebook <- NA
df$reddit <- NA
df$google <- NA
df$mail <- NA
df$title  <- NA
df$postTags <- NA
df$time <- NA
if (limit == 0) {
l <- length(links)
} else {
l <- limit
}
time <- Sys.time()
for(i in seq.int(l)) {
link <- links[[i]]
html <- read_html(link)
data <- html_text(html_nodes(html, selector))
#Social networks
df$twitter[i] <- data[1]
df$linkedin[i] <- data[2]
df$facebook[i] <- data[3]
df$reddit[i] <- data[4]
df$google[i] <- data[5]
df$mail[i] <- data[6]
#Title
df$title[i] <- data[6]
#Tags
if (length(data) > 8) {
df$postTags[i] <- paste(data[9:length(data)], collapse = ",")
}
#Stats
stats <- unlist(strsplit(data[8], "[[:space:]]"))
#Views
df$views[i] <- as.integer(stats[7])
#Comments
df$comments[i] <- as.integer(stats[12])
#Time
timestamp <- as.integer(Sys.time())
if (stats[2] == "hours" || stats[2] == "hour") {
hours <- as.integer(stats[1])
df$time[i] <- time - hours * 3600
} else if (stats[2] == "minutes" || stats[2] == "minute") {
minutes <- as.integer(stats[1])
df$time[i] <- time - minutes * 60
} else {
year <- as.integer(stats[3])
month <- which(months == stats[2])
day <- as.integer(stats[1])
date <- ISOdate(year, month, day)
df$time[i] <- as.integer(date)
}
#Progress information
if (i %% step == 0) {
avgTime <- (as.integer(Sys.time()) - startTime) / i
remainingTime <- avgTime * (l-i)
seconds <- round(remainingTime %% 60)
minutes <- round((remainingTime - seconds) / 60)
progress_bar <- paste(c("[", lapply(seq.int(25), function(x, progress) {
if (x <= progress) {
"#"
} else {
" "
}
}, round(i/l*25)), "] "), sep = "", collapse = "")
stepindicator <- sprintf("%s/%s", i, l)
timeEstimate <- sprintf(" (%s minutes %s seconds)", minutes, seconds)
print(paste(c( progress_bar, stepindicator, timeEstimate), sep = " ", collapse = ""))
}
}
df <- subset(df, !is.na(title))
df$id <- seq.int(length(df$url))
if (beep) {
beep()
}
df[1:length(df$title),]
}
go <- function() {
links <- crawler(2, url, path)
df <- parser(links, 100)
}
links <- crawler(2, url, path)
df <- parser(links, step = 10, beep = T)
library(RMySQL)
#Connection to the database
con <- dbConnect(MySQL(), user="skaizen2", password="kzm38gw35f", dbname="webcrawler", host="localhost")
#Overwrite table
dbWriteTable(conn=con, name=tablename, value=df, row.names = FALSE, overwrite = TRUE)
tablename <- "webcrawler"
#Overwrite table
dbWriteTable(conn=con, name=tablename, value=df, row.names = FALSE, overwrite = TRUE)
source('~/GitHub/Skaizen/WebCrawler.R')
df$twitter -> as.integer(df$twitter)
df$twitter <- as.integer(df$twitter)
df$linkedin -> as.integer(df$twitter)
df$linkedin <- as.integer(df$linkedin)
df$facebook <- as.integer(df$facebook)
df$reddit <- as.integer(df$reddit)
df$google <- as.integer(df$google)
df$mail <- as.integer(df$mail)
View(df)
#Overwrite table
dbWriteTable(conn=con, name=tablename, value=df, row.names = FALSE, overwrite = TRUE)
